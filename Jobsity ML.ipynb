{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "from fastai.structured import add_datepart\n",
    "\n",
    "rawData = pandas.read_csv('Interview.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                            int64\n",
       "has_permission                    int64\n",
       "unscheduled                       int64\n",
       "can_call                          int64\n",
       "can_have_alt_number               int64\n",
       "has_cv_print                      int64\n",
       "venue                             int64\n",
       "call_letter_shared                int64\n",
       "expected_attendance               int64\n",
       "observed_attendance               int64\n",
       "Year                              int64\n",
       "Month                             int64\n",
       "Week                              int64\n",
       "Day                               int64\n",
       "Dayofweek                         int64\n",
       "Dayofyear                         int64\n",
       "Is_month_end                       bool\n",
       "Is_month_start                     bool\n",
       "Is_quarter_end                     bool\n",
       "Is_quarter_start                   bool\n",
       "Is_year_end                        bool\n",
       "Is_year_start                      bool\n",
       "Elapsed                           int64\n",
       "skillset_accounting_operations    uint8\n",
       "skillset_als_testing              uint8\n",
       "skillset_aml                      uint8\n",
       "skillset_analyst                  uint8\n",
       "skillset_analytical_r&d           uint8\n",
       "skillset_automation               uint8\n",
       "skillset_banking_operations       uint8\n",
       "                                  ...  \n",
       "native_loc_faizabad               uint8\n",
       "native_loc_ghaziabad              uint8\n",
       "native_loc_gurgaon                uint8\n",
       "native_loc_hissar                 uint8\n",
       "native_loc_hosur                  uint8\n",
       "native_loc_hyderabad              uint8\n",
       "native_loc_kanpur                 uint8\n",
       "native_loc_kolkata                uint8\n",
       "native_loc_kurnool                uint8\n",
       "native_loc_lucknow                uint8\n",
       "native_loc_mumbai                 uint8\n",
       "native_loc_mysore                 uint8\n",
       "native_loc_nagercoil              uint8\n",
       "native_loc_noida                  uint8\n",
       "native_loc_panjim                 uint8\n",
       "native_loc_patna                  uint8\n",
       "native_loc_pondicherry            uint8\n",
       "native_loc_pune                   uint8\n",
       "native_loc_salem                  uint8\n",
       "native_loc_tanjore                uint8\n",
       "native_loc_tirupati               uint8\n",
       "native_loc_trichy                 uint8\n",
       "native_loc_trivandrum             uint8\n",
       "native_loc_tuticorin              uint8\n",
       "native_loc_vellore                uint8\n",
       "native_loc_vijayawada             uint8\n",
       "native_loc_visakapatinam          uint8\n",
       "native_loc_warangal               uint8\n",
       "marital_status_married            uint8\n",
       "marital_status_single             uint8\n",
       "Length: 181, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_cleaned_date(date):\n",
    "    \"\"\"\n",
    "    Return datetime object from a string\n",
    "    \"\"\"\n",
    "    date = str(date).strip()\n",
    "\n",
    "    if '&' in date:\n",
    "        date = date.split('&')[0].strip()\n",
    "\n",
    "    # Since there are a lot of formats in the data, need to handle all the possible options\n",
    "    date_formats = [\n",
    "        '%d.%m.%Y', '%d.%m.%y', '%d.%m.%y', '%d-%m-%Y', '%d/%m/%y', '%d/%m/%Y', '%d %b %y', '%d-%b -%y',\n",
    "        '%d – %b-%y', '%d -%b -%y'\n",
    "    ]\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date, date_format)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "#We remove ID column to reduce overfitting and the final columns filled with NaNs\n",
    "cleanData = rawData.drop(['Name(Cand ID)', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27'], axis = 1)\n",
    "\n",
    "#This row is full of NaNs we should remove it.\n",
    "cleanData = cleanData.drop(1233)\n",
    "\n",
    "#Change column names for easier reference\n",
    "cleanData.columns = [\n",
    "    'date',\n",
    "    'client',\n",
    "    'industry',\n",
    "    'loc',\n",
    "    'pos',\n",
    "    'skillset',\n",
    "    'interview_type',\n",
    "    'gender',\n",
    "    'current_loc',\n",
    "    'job_loc',\n",
    "    'venue_loc',\n",
    "    'native_loc',\n",
    "    'has_permission',\n",
    "    'unscheduled',\n",
    "    'can_call',\n",
    "    'can_have_alt_number',\n",
    "    'has_cv_print',\n",
    "    'venue',\n",
    "    'call_letter_shared',\n",
    "    'expected_attendance',\n",
    "    'observed_attendance',\n",
    "    'marital_status']\n",
    "\n",
    "#Make everything lower case and strip whitespace padding for more uniform data\n",
    "cleanData = pandas.concat([cleanData[c].astype(str).str.lower() for c in cleanData.columns], axis = 1)\n",
    "cleanData = pandas.concat([cleanData[c].astype(str).str.strip() for c in cleanData.columns], axis = 1)\n",
    "\n",
    "#Fix date formats\n",
    "cleanData['date'] = cleanData['date'].map(get_cleaned_date)\n",
    "\n",
    "#We know the date range should be between sep 2014 and jan 2017 so we eliminate all records outside of that range.\n",
    "#As there are some outliers into the future.\n",
    "#NOTE: we should try to calculate mean +- 1SD to possibly substitute these outliers and check if we get better results.\n",
    "cleanData = cleanData[(cleanData['date'] >'2014-09-01') & (cleanData['date'] < '2017-01-31')]\n",
    "\n",
    "#Lets extract features from the date field that are easier to understand by most algorithms\n",
    "add_datepart(cleanData, 'date', drop=True)\n",
    "#NOTE: Might possibly want to add days to nearest holiday and days from nearest holiday.\n",
    "#But that needs some research as to India's holidays and potential usefulness of such features\n",
    "\n",
    "\n",
    "#Merge similar values standard chartered bank and standard chartered bank chennai.\n",
    "#job location is specified in another column.\n",
    "cleanData['client'].replace('standard chartered bank chennai', 'standard chartered bank', inplace = True)\n",
    "\n",
    "#aon hewitt, hewitt and aon hewitt gurgaon seem to be the same company, we merge those too\n",
    "cleanData['client'].replace(['hewitt', 'aon hewitt gurgaon'], 'aon hewitt', inplace = True)\n",
    "\n",
    "#Merge similar industry items\n",
    "cleanData['industry'].replace(['it products and services', 'it services'], 'it', inplace = True)\n",
    "\n",
    "#Fix interview types\n",
    "cleanData['interview_type'].replace(['scheduled walk in', 'sceduled walkin'], 'scheduled walkin', inplace=True)\n",
    "\n",
    "#Fix location fields\n",
    "cleanData['loc'].replace(['- cochin-', 'gurgaonr'], ['cochin', 'gurgaon'], inplace = True)\n",
    "cleanData['current_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['job_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['venue_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('delhi /ncr', 'delhi', inplace=True)\n",
    "\n",
    "#We will assume that the time means that the candidate is expected at that time\n",
    "time_pattern = '\\d?\\d[.:]\\d\\d [ap]m'\n",
    "cleanData['expected_attendance'].replace(time_pattern, 'yes', regex=True, inplace=True)\n",
    "\n",
    "#Normalize yes/no/na fields\n",
    "features= ['has_permission', 'unscheduled', 'can_call', 'can_have_alt_number', 'has_cv_print', 'venue', 'call_letter_shared', 'expected_attendance', 'observed_attendance']\n",
    "no = ['no', 'not yet', 'no dont', 'no- will take it soon', 'no i have only thi number', 'no- i need to check']\n",
    "na = ['na', 'nan', 'not sure', 'cant say', 'yet to confirm', 'need to check', 'yet to check', 'havent checked', 'uncertain']\n",
    "\n",
    "#We use 1, 0, -1 for 'yes', 'na', 'no' respectively as such values are easier for most algorithms to make inferences\n",
    "for feature in features:\n",
    "    cleanData[feature].replace(no, -1, inplace=True)\n",
    "    cleanData[feature].replace(na, 0, inplace=True)\n",
    "    cleanData[feature].replace('yes', 1, inplace=True)\n",
    "    \n",
    "#Normalize gender column.\n",
    "#We use a -1,1 range as it is recommended for gender data.\n",
    "cleanData['gender'].replace('male', 1, inplace=True)\n",
    "cleanData['gender'].replace('female', -1, inplace=True)\n",
    "\n",
    "\n",
    "#We drop values that look like dateTime as they make no sense in the skillset column.\n",
    "filterSeries = cleanData['skillset'].str.contains(time_pattern, regex=True)\n",
    "cleanData = cleanData[~filterSeries]\n",
    "\n",
    "#Let's give the field some somblance of an uniform format for easier processing\n",
    "cleanData['skillset'].replace([\n",
    "    '/', ' ?, ?', ' developer', 'r & d', 'sccm ?- ?', ' – ra'\n",
    "], [\n",
    "    ',', ',', '',  'r&d', 'sccm,',  ''\n",
    "], inplace=True, regex=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'cdd kyc',\n",
    "    'java j2ee',\n",
    "    'oracle plsql',\n",
    "    'core java',\n",
    "    'senior software engineer-mednet',\n",
    "    'sr automation testing',\n",
    "    'tech lead-mednet',\n",
    "    'ra publishing',\n",
    "    'java jsf',\n",
    "    'java,j2ee,core java',\n",
    "    'java tech lead',\n",
    "    'automation testing java',\n",
    "    '- sapbo,informatica',\n",
    "    'production support - sccm',\n",
    "    'tech lead- mednet',\n",
    "    'technical lead',\n",
    "    'senior analyst',\n",
    "    'sccm – sharepoint',\n",
    "    'sccm – sql',\n",
    "    'tl',\n",
    "    'sccm,(network,sharepoint,ms exchange)',\n",
    "    'java-sas',\n",
    "    'lcm -manager',\n",
    "    'basesas_program,reporting'\n",
    "], [\n",
    "    'cdd,kyc',\n",
    "    'java,j2ee',\n",
    "    'plsql',\n",
    "    'java',\n",
    "    'senior,developer,mednet',\n",
    "    'senior,automation,testing',\n",
    "    'tech lead,mednet',\n",
    "    'publishing',\n",
    "    'java,jsf',\n",
    "    'java,j2ee',\n",
    "    'java,tech lead',\n",
    "    'java,automation,testing',\n",
    "    'sapbo,informatica',\n",
    "    'sccm,production support',\n",
    "    'tech lead,mednet',\n",
    "    'tech lead',\n",
    "    'senior,analyst',\n",
    "    'sccm,sharepoint',\n",
    "    'sccm,sql',\n",
    "    'tech lead',\n",
    "    'sccm,network,sharepoint,ms_exchange',\n",
    "    'java,sas',\n",
    "    'lcm manager',\n",
    "    'baseSAS'\n",
    "    \n",
    "], inplace=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'lending&liablities',\n",
    "    'l & l',\n",
    "    'lending & liability'\n",
    "], 'lending and liabilities', inplace=True)\n",
    "cleanData['skillset'].replace(['biosimiliars', 'biosimillar'], 'biosimilars', inplace=True)\n",
    "cleanData['skillset'].replace(' ',  '_', regex=True, inplace=True)\n",
    "\n",
    "#Lets binarize each item into categories\n",
    "skillset = cleanData.skillset.str.split(',', expand=True).stack()\n",
    "dummies = pandas.get_dummies(skillset, prefix='skillset').groupby(level=0).sum()\n",
    "\n",
    "cleanData = cleanData.join(dummies)\n",
    "\n",
    "#We drop skillset as it is now encoded\n",
    "cleanData.drop('skillset', axis=1, inplace=True)\n",
    "\n",
    "#Encode other categorical features using oneHot\n",
    "features = ['loc', 'pos', 'client', 'industry', 'interview_type', 'current_loc', 'job_loc', 'venue_loc', 'native_loc', 'marital_status']\n",
    "\n",
    "for feature in features:\n",
    "    dummies = pandas.get_dummies(cleanData[feature], prefix=feature)\n",
    "    cleanData.drop(feature, axis=1, inplace=True)\n",
    "    cleanData = cleanData.join(dummies)\n",
    "\n",
    "\n",
    "cleanData.to_csv('excel_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
