{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "from fastai.structured import add_datepart\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import feature_selection\n",
    "\n",
    "rawData = pandas.read_csv('Interview.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cleaned_date(date):\n",
    "    \"\"\"\n",
    "    Return datetime object from a string\n",
    "    \"\"\"\n",
    "    date = str(date).strip()\n",
    "\n",
    "    if '&' in date:\n",
    "        date = date.split('&')[0].strip()\n",
    "\n",
    "    # Since there are a lot of formats in the data, need to handle all the possible options\n",
    "    date_formats = [\n",
    "        '%d.%m.%Y', '%d.%m.%y', '%d.%m.%y', '%d-%m-%Y', '%d/%m/%y', '%d/%m/%Y', '%d %b %y', '%d-%b -%y',\n",
    "        '%d – %b-%y', '%d -%b -%y'\n",
    "    ]\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date, date_format)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "#We remove ID column to reduce overfitting and the final columns filled with NaNs\n",
    "cleanData = rawData.drop(['Name(Cand ID)', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27'], axis = 1)\n",
    "\n",
    "#This row is full of NaNs we should remove it.\n",
    "cleanData = cleanData.drop(1233)\n",
    "\n",
    "#Change column names for easier reference\n",
    "cleanData.columns = [\n",
    "    'date',\n",
    "    'client',\n",
    "    'industry',\n",
    "    'loc',\n",
    "    'pos',\n",
    "    'skillset',\n",
    "    'interview_type',\n",
    "    'gender',\n",
    "    'current_loc',\n",
    "    'job_loc',\n",
    "    'venue_loc',\n",
    "    'native_loc',\n",
    "    'has_permission',\n",
    "    'unscheduled',\n",
    "    'can_call',\n",
    "    'can_have_alt_number',\n",
    "    'has_cv_print',\n",
    "    'venue',\n",
    "    'call_letter_shared',\n",
    "    'expected_attendance',\n",
    "    'observed_attendance',\n",
    "    'marital_status']\n",
    "\n",
    "#Make everything lower case and strip whitespace padding for more uniform data\n",
    "cleanData = pandas.concat([cleanData[c].astype(str).str.lower() for c in cleanData.columns], axis = 1)\n",
    "cleanData = pandas.concat([cleanData[c].astype(str).str.strip() for c in cleanData.columns], axis = 1)\n",
    "\n",
    "#Fix date formats\n",
    "cleanData['date'] = cleanData['date'].map(get_cleaned_date)\n",
    "\n",
    "#We know the date range should be between sep 2014 and jan 2017 so we eliminate all records outside of that range.\n",
    "#As there are some outliers into the future.\n",
    "#NOTE: we should try to calculate mean +- 1SD to possibly substitute these outliers and check if we get better results.\n",
    "cleanData = cleanData[(cleanData['date'] >'2014-09-01') & (cleanData['date'] < '2017-01-31')]\n",
    "\n",
    "#Lets extract features from the date field that are easier to understand by most algorithms\n",
    "add_datepart(cleanData, 'date', drop=True)\n",
    "#NOTE: Might possibly want to add days to nearest holiday and days from nearest holiday.\n",
    "#But that needs some research as to India's holidays and potential usefulness of such features\n",
    "\n",
    "\n",
    "#Merge similar values standard chartered bank and standard chartered bank chennai.\n",
    "#job location is specified in another column.\n",
    "cleanData['client'].replace('standard chartered bank chennai', 'standard chartered bank', inplace = True)\n",
    "\n",
    "#aon hewitt, hewitt and aon hewitt gurgaon seem to be the same company, we merge those too\n",
    "cleanData['client'].replace(['hewitt', 'aon hewitt gurgaon'], 'aon hewitt', inplace = True)\n",
    "\n",
    "#Merge similar industry items\n",
    "cleanData['industry'].replace(['it products and services', 'it services'], 'it', inplace = True)\n",
    "\n",
    "#Fix interview types\n",
    "cleanData['interview_type'].replace(['scheduled walk in', 'sceduled walkin'], 'scheduled walkin', inplace=True)\n",
    "\n",
    "#Fix location fields\n",
    "cleanData['loc'].replace(['- cochin-', 'gurgaonr'], ['cochin', 'gurgaon'], inplace = True)\n",
    "cleanData['current_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['job_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['venue_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('delhi /ncr', 'delhi', inplace=True)\n",
    "\n",
    "#We will assume that the time means that the candidate is expected at that time\n",
    "time_pattern = '\\d?\\d[.:]\\d\\d [ap]m'\n",
    "cleanData['expected_attendance'].replace(time_pattern, 'yes', regex=True, inplace=True)\n",
    "\n",
    "#Normalize yes/no/na fields\n",
    "features= ['has_permission', 'unscheduled', 'can_call', 'can_have_alt_number', 'has_cv_print', 'venue', 'call_letter_shared', 'expected_attendance', 'observed_attendance']\n",
    "no = ['no', 'not yet', 'no dont', 'no- will take it soon', 'no i have only thi number', 'no- i need to check']\n",
    "na = ['na', 'nan', 'not sure', 'cant say', 'yet to confirm', 'need to check', 'yet to check', 'havent checked', 'uncertain']\n",
    "\n",
    "#We use 1, 0, -1 for 'yes', 'na', 'no' respectively as such values are easier for most algorithms to make inferences\n",
    "for feature in features:\n",
    "    cleanData[feature].replace(no, -1, inplace=True)\n",
    "    cleanData[feature].replace(na, 0, inplace=True)\n",
    "    cleanData[feature].replace('yes', 1, inplace=True)\n",
    "    \n",
    "#Normalize gender column.\n",
    "#We use a -1,1 range as it is recommended for gender data.\n",
    "cleanData['gender'].replace('male', 1, inplace=True)\n",
    "cleanData['gender'].replace('female', -1, inplace=True)\n",
    "\n",
    "\n",
    "#We drop values that look like dateTime as they make no sense in the skillset column.\n",
    "filterSeries = cleanData['skillset'].str.contains(time_pattern, regex=True)\n",
    "cleanData = cleanData[~filterSeries]\n",
    "\n",
    "#Let's give the field some somblance of an uniform format for easier processing\n",
    "cleanData['skillset'].replace([\n",
    "    '/', ' ?, ?', ' developer', 'r & d', 'sccm ?- ?', ' – ra'\n",
    "], [\n",
    "    ',', ',', '',  'r&d', 'sccm,',  ''\n",
    "], inplace=True, regex=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'cdd kyc',\n",
    "    'java j2ee',\n",
    "    'oracle plsql',\n",
    "    'core java',\n",
    "    'senior software engineer-mednet',\n",
    "    'sr automation testing',\n",
    "    'tech lead-mednet',\n",
    "    'ra publishing',\n",
    "    'java jsf',\n",
    "    'java,j2ee,core java',\n",
    "    'java tech lead',\n",
    "    'automation testing java',\n",
    "    '- sapbo,informatica',\n",
    "    'production support - sccm',\n",
    "    'tech lead- mednet',\n",
    "    'technical lead',\n",
    "    'senior analyst',\n",
    "    'sccm – sharepoint',\n",
    "    'sccm – sql',\n",
    "    'tl',\n",
    "    'sccm,(network,sharepoint,ms exchange)',\n",
    "    'java-sas',\n",
    "    'lcm -manager',\n",
    "    'basesas_program,reporting'\n",
    "], [\n",
    "    'cdd,kyc',\n",
    "    'java,j2ee',\n",
    "    'plsql',\n",
    "    'java',\n",
    "    'senior,developer,mednet',\n",
    "    'senior,automation,testing',\n",
    "    'tech lead,mednet',\n",
    "    'publishing',\n",
    "    'java,jsf',\n",
    "    'java,j2ee',\n",
    "    'java,tech lead',\n",
    "    'java,automation,testing',\n",
    "    'sapbo,informatica',\n",
    "    'sccm,production support',\n",
    "    'tech lead,mednet',\n",
    "    'tech lead',\n",
    "    'senior,analyst',\n",
    "    'sccm,sharepoint',\n",
    "    'sccm,sql',\n",
    "    'tech lead',\n",
    "    'sccm,network,sharepoint,ms_exchange',\n",
    "    'java,sas',\n",
    "    'lcm manager',\n",
    "    'baseSAS'\n",
    "    \n",
    "], inplace=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'lending&liablities',\n",
    "    'l & l',\n",
    "    'lending & liability'\n",
    "], 'lending and liabilities', inplace=True)\n",
    "cleanData['skillset'].replace(['biosimiliars', 'biosimillar'], 'biosimilars', inplace=True)\n",
    "cleanData['skillset'].replace(' ',  '_', regex=True, inplace=True)\n",
    "\n",
    "#Lets binarize each item into categories\n",
    "skillset = cleanData.skillset.str.split(',', expand=True).stack()\n",
    "dummies = pandas.get_dummies(skillset, prefix='skillset').groupby(level=0).sum()\n",
    "\n",
    "cleanData = cleanData.join(dummies)\n",
    "\n",
    "#We drop skillset as it is now encoded\n",
    "cleanData.drop('skillset', axis=1, inplace=True)\n",
    "\n",
    "#Encode other categorical features using oneHot\n",
    "features = ['loc', 'pos', 'client', 'industry', 'interview_type', 'current_loc', 'job_loc', 'venue_loc', 'native_loc', 'marital_status']\n",
    "\n",
    "for feature in features:\n",
    "    dummies = pandas.get_dummies(cleanData[feature], prefix=feature)\n",
    "    cleanData.drop(feature, axis=1, inplace=True)\n",
    "    cleanData = cleanData.join(dummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target_field, frac=0.75, random_state=42):\n",
    "    #Fit the normalizer with our unseparated data minus the target column\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data.drop(target_field, axis=1))\n",
    "\n",
    "    y = data.observed_attendance\n",
    "    X = data.drop(target_field, axis=1)\n",
    "\n",
    "    #Separate data into training and test sets\n",
    "    train = data.sample(frac=frac, random_state=random_state)\n",
    "    test = data.loc[~data.index.isin(train.index),  :]\n",
    "\n",
    "    #exctract the target feature\n",
    "    y1 = train.observed_attendance\n",
    "    x1 = train.drop(target_field, axis=1)\n",
    "    y2 = test.observed_attendance\n",
    "    x2 = test.drop(target_field, axis=1)\n",
    "    \n",
    "    #Normalize separated data\n",
    "    #x1 = scaler.transform(x1)\n",
    "    #x2 = scaler.transform(x2)\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model, x1, x2, y1, y2):\n",
    "    model.fit(xTrain, yTrain)\n",
    "    print('Train: ', model.score(x1, y1))\n",
    "    print('Test: ', model.score(x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, yTrain, xTest, yTest = split_data(cleanData, 'observed_attendance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.28114687145336137\n",
      "Test:  0.08947106869333588\n",
      "Reducing features...\n",
      "Train:  0.013744223153244639\n",
      "Test:  0.0005435161417249867\n",
      "Optimal number of features : 1\n"
     ]
    }
   ],
   "source": [
    "# Try linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv, xTrain, xTest, yTrain, yTest)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.6355653128430296\n",
      "Test:  0.625\n",
      "Reducing features...\n",
      "Train:  0.6355653128430296\n",
      "Test:  0.625\n",
      "Optimal number of features : 180\n"
     ]
    }
   ],
   "source": [
    "# Try logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# We set random_state for reproceability\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv, xTrain, xTest, yTrain, yTest)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.849615806805708\n",
      "Test:  0.6710526315789473\n",
      "Reducing features...\n",
      "Train:  0.7145993413830956\n",
      "Test:  0.6907894736842105\n",
      "Optimal number of features : 1\n"
     ]
    }
   ],
   "source": [
    "#Try decision trees\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv, xTrain, xTest, yTrain, yTest)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.36443468715697036\n",
      "Test:  0.375\n",
      "Reducing features...\n",
      "Train:  0.6355653128430296\n",
      "Test:  0.625\n",
      "Optimal number of features : 165\n"
     ]
    }
   ],
   "source": [
    "#Try SVM Linear Classification, we preffer classification over regression because this is a classification problem\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.LinearSVC()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv, xTrain, xTest, yTrain, yTest)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  -5.511326370532864e+16\n",
      "Test:  -2.217932067861632e+16\n"
     ]
    }
   ],
   "source": [
    "#Try K-means\n",
    "#we only use this because it is required in the challenge, but we do not expect this to solve the actual problem\n",
    "#because this is just a clustering algorithm.\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2) #We set n_clusters=2 because we only care about 2 possible outcomes.\n",
    "\n",
    "test(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
