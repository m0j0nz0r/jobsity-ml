{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "# from fastai.structured import add_datepart\n",
    "from sklearn import feature_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rawData = pd.read_csv('Interview.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    \"\"\"\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "        \n",
    "def get_cleaned_date(date):\n",
    "    \"\"\"\n",
    "    Return datetime object from a string\n",
    "    \"\"\"\n",
    "    date = str(date).strip()\n",
    "\n",
    "    if '&' in date:\n",
    "        date = date.split('&')[0].strip()\n",
    "\n",
    "    # Since there are a lot of formats in the data, need to handle all the possible options\n",
    "    date_formats = [\n",
    "        '%d.%m.%Y', '%d.%m.%y', '%d.%m.%y', '%d-%m-%Y', '%d/%m/%y', '%d/%m/%Y', '%d %b %y', '%d-%b -%y',\n",
    "        '%d – %b-%y', '%d -%b -%y'\n",
    "    ]\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date, date_format)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "#We remove ID column to reduce overfitting and the final columns filled with NaNs\n",
    "cleanData = rawData.drop(['Name(Cand ID)', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27'], axis = 1)\n",
    "\n",
    "#This row is full of NaNs we should remove it.\n",
    "cleanData = cleanData.drop(1233)\n",
    "\n",
    "#Change column names for easier reference\n",
    "cleanData.columns = [\n",
    "    'date',\n",
    "    'client',\n",
    "    'industry',\n",
    "    'loc',\n",
    "    'pos',\n",
    "    'skillset',\n",
    "    'interview_type',\n",
    "    'gender',\n",
    "    'current_loc',\n",
    "    'job_loc',\n",
    "    'venue_loc',\n",
    "    'native_loc',\n",
    "    'has_permission',\n",
    "    'unscheduled',\n",
    "    'can_call',\n",
    "    'can_have_alt_number',\n",
    "    'has_cv_print',\n",
    "    'venue',\n",
    "    'call_letter_shared',\n",
    "    'expected_attendance',\n",
    "    'observed_attendance',\n",
    "    'marital_status']\n",
    "\n",
    "#Make everything lower case and strip whitespace padding for more uniform data\n",
    "cleanData = pd.concat([cleanData[c].astype(str).str.lower() for c in cleanData.columns], axis = 1)\n",
    "cleanData = pd.concat([cleanData[c].astype(str).str.strip() for c in cleanData.columns], axis = 1)\n",
    "\n",
    "#Fix date formats\n",
    "cleanData['date'] = cleanData['date'].map(get_cleaned_date)\n",
    "\n",
    "#We know the date range should be between sep 2014 and jan 2017 so we eliminate all records outside of that range.\n",
    "#As there are some outliers into the future.\n",
    "#NOTE: we should try to calculate mean +- 1SD to possibly substitute these outliers and check if we get better results.\n",
    "cleanData = cleanData[(cleanData['date'] >'2014-09-01') & (cleanData['date'] < '2017-01-31')]\n",
    "\n",
    "#Lets extract features from the date field that are easier to understand by most algorithms\n",
    "add_datepart(cleanData, 'date', drop=True)\n",
    "#NOTE: Might possibly want to add days to nearest holiday and days from nearest holiday.\n",
    "#But that needs some research as to India's holidays and potential usefulness of such features\n",
    "\n",
    "\n",
    "#Merge similar values standard chartered bank and standard chartered bank chennai.\n",
    "#job location is specified in another column.\n",
    "cleanData['client'].replace('standard chartered bank chennai', 'standard chartered bank', inplace = True)\n",
    "\n",
    "#aon hewitt, hewitt and aon hewitt gurgaon seem to be the same company, we merge those too\n",
    "cleanData['client'].replace(['hewitt', 'aon hewitt gurgaon'], 'aon hewitt', inplace = True)\n",
    "\n",
    "#Merge similar industry items\n",
    "cleanData['industry'].replace(['it products and services', 'it services'], 'it', inplace = True)\n",
    "\n",
    "#Fix interview types\n",
    "cleanData['interview_type'].replace(['scheduled walk in', 'sceduled walkin'], 'scheduled walkin', inplace=True)\n",
    "\n",
    "#Fix location fields\n",
    "cleanData['loc'].replace(['- cochin-', 'gurgaonr'], ['cochin', 'gurgaon'], inplace = True)\n",
    "cleanData['current_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['job_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['venue_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('- cochin-', 'cochin', inplace=True)\n",
    "cleanData['native_loc'].replace('delhi /ncr', 'delhi', inplace=True)\n",
    "\n",
    "#We will assume that the time means that the candidate is expected at that time\n",
    "time_pattern = '\\d?\\d[.:]\\d\\d [ap]m'\n",
    "cleanData['expected_attendance'].replace(time_pattern, 'yes', regex=True, inplace=True)\n",
    "\n",
    "#Normalize yes/no/na fields\n",
    "features= ['has_permission', 'unscheduled', 'can_call', 'can_have_alt_number', 'has_cv_print', 'venue', 'call_letter_shared', 'expected_attendance', 'observed_attendance']\n",
    "no = ['no', 'not yet', 'no dont', 'no- will take it soon', 'no i have only thi number', 'no- i need to check']\n",
    "na = ['na', 'nan', 'not sure', 'cant say', 'yet to confirm', 'need to check', 'yet to check', 'havent checked', 'uncertain']\n",
    "\n",
    "#We use 1, 0, -1 for 'yes', 'na', 'no' respectively as such values are easier for most algorithms to make inferences\n",
    "for feature in features:\n",
    "    cleanData[feature].replace(no, -1, inplace=True)\n",
    "    cleanData[feature].replace(na, 0, inplace=True)\n",
    "    cleanData[feature].replace('yes', 1, inplace=True)\n",
    "    \n",
    "#Normalize gender column.\n",
    "#We use a -1,1 range as it is recommended for gender data.\n",
    "cleanData['gender'].replace('male', 1, inplace=True)\n",
    "cleanData['gender'].replace('female', -1, inplace=True)\n",
    "\n",
    "\n",
    "#We drop values that look like dateTime as they make no sense in the skillset column.\n",
    "filterSeries = cleanData['skillset'].str.contains(time_pattern, regex=True)\n",
    "cleanData = cleanData[~filterSeries]\n",
    "\n",
    "#Let's give the field some somblance of an uniform format for easier processing\n",
    "cleanData['skillset'].replace([\n",
    "    '/', ' ?, ?', ' developer', 'r & d', 'sccm ?- ?', ' – ra'\n",
    "], [\n",
    "    ',', ',', '',  'r&d', 'sccm,',  ''\n",
    "], inplace=True, regex=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'cdd kyc',\n",
    "    'java j2ee',\n",
    "    'oracle plsql',\n",
    "    'core java',\n",
    "    'senior software engineer-mednet',\n",
    "    'sr automation testing',\n",
    "    'tech lead-mednet',\n",
    "    'ra publishing',\n",
    "    'java jsf',\n",
    "    'java,j2ee,core java',\n",
    "    'java tech lead',\n",
    "    'automation testing java',\n",
    "    '- sapbo,informatica',\n",
    "    'production support - sccm',\n",
    "    'tech lead- mednet',\n",
    "    'technical lead',\n",
    "    'senior analyst',\n",
    "    'sccm – sharepoint',\n",
    "    'sccm – sql',\n",
    "    'tl',\n",
    "    'sccm,(network,sharepoint,ms exchange)',\n",
    "    'java-sas',\n",
    "    'lcm -manager',\n",
    "    'basesas_program,reporting'\n",
    "], [\n",
    "    'cdd,kyc',\n",
    "    'java,j2ee',\n",
    "    'plsql',\n",
    "    'java',\n",
    "    'senior,developer,mednet',\n",
    "    'senior,automation,testing',\n",
    "    'tech lead,mednet',\n",
    "    'publishing',\n",
    "    'java,jsf',\n",
    "    'java,j2ee',\n",
    "    'java,tech lead',\n",
    "    'java,automation,testing',\n",
    "    'sapbo,informatica',\n",
    "    'sccm,production support',\n",
    "    'tech lead,mednet',\n",
    "    'tech lead',\n",
    "    'senior,analyst',\n",
    "    'sccm,sharepoint',\n",
    "    'sccm,sql',\n",
    "    'tech lead',\n",
    "    'sccm,network,sharepoint,ms_exchange',\n",
    "    'java,sas',\n",
    "    'lcm manager',\n",
    "    'baseSAS'\n",
    "    \n",
    "], inplace=True)\n",
    "cleanData['skillset'].replace([\n",
    "    'lending&liablities',\n",
    "    'l & l',\n",
    "    'lending & liability'\n",
    "], 'lending and liabilities', inplace=True)\n",
    "cleanData['skillset'].replace(['biosimiliars', 'biosimillar'], 'biosimilars', inplace=True)\n",
    "cleanData['skillset'].replace(' ',  '_', regex=True, inplace=True)\n",
    "\n",
    "#Lets binarize each item into categories\n",
    "skillset = cleanData.skillset.str.split(',', expand=True).stack()\n",
    "dummies = pd.get_dummies(skillset, prefix='skillset').groupby(level=0).sum()\n",
    "\n",
    "cleanData = cleanData.join(dummies)\n",
    "\n",
    "#We drop skillset as it is now encoded\n",
    "cleanData.drop('skillset', axis=1, inplace=True)\n",
    "\n",
    "#Encode other categorical features using oneHot\n",
    "features = ['loc', 'pos', 'client', 'industry', 'interview_type', 'current_loc', 'job_loc', 'venue_loc', 'native_loc', 'marital_status']\n",
    "\n",
    "for feature in features:\n",
    "    dummies = pd.get_dummies(cleanData[feature], prefix=feature)\n",
    "    cleanData.drop(feature, axis=1, inplace=True)\n",
    "    cleanData = cleanData.join(dummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target_field, frac=0.75, random_state=42):\n",
    "    Y = data.observed_attendance\n",
    "    X = data.drop(target_field, axis=1)\n",
    "\n",
    "    #Fit the normalizer with our unseparated data minus the target column\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit_transform(X)\n",
    "    \n",
    "    #Separate data into training and test sets\n",
    "    return train_test_split(X, Y, test_size=frac, random_state=random_state)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model):\n",
    "    model.fit(xTrain, yTrain)\n",
    "    print('Train: ', model.score(xTrain, yTrain))\n",
    "    print('Test: ', model.score(xTest, yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest, xTrain, yTest, yTrain = split_data(cleanData, 'observed_attendance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.27847151452122887\n",
      "Test:  0.099680483347796\n",
      "Reducing features...\n",
      "Train:  0.022734419521123983\n",
      "Test:  -0.0010158778680553482\n",
      "Optimal number of features : 6\n",
      "Selected features: %s Index(['Year', 'Week', 'Dayofweek', 'Dayofyear', 'skillset_cots',\n",
      "       'industry_telecom'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Try linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(\"Selected features: %s\", xTrain.columns[rfecv.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.6348684210526315\n",
      "Test:  0.6270627062706271\n",
      "Reducing features...\n",
      "Train:  0.6348684210526315\n",
      "Test:  0.6270627062706271\n",
      "Optimal number of features : 180\n",
      "Selected features: %s Index(['gender', 'has_permission', 'unscheduled', 'can_call',\n",
      "       'can_have_alt_number', 'has_cv_print', 'venue', 'call_letter_shared',\n",
      "       'expected_attendance', 'Year',\n",
      "       ...\n",
      "       'native_loc_tirupati', 'native_loc_trichy', 'native_loc_trivandrum',\n",
      "       'native_loc_tuticorin', 'native_loc_vellore', 'native_loc_vijayawada',\n",
      "       'native_loc_visakapatinam', 'native_loc_warangal',\n",
      "       'marital_status_married', 'marital_status_single'],\n",
      "      dtype='object', length=180)\n"
     ]
    }
   ],
   "source": [
    "# Try logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# We set random_state for reproceability\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(\"Selected features: %s\", xTrain.columns[rfecv.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.8497807017543859\n",
      "Test:  0.6666666666666666\n",
      "Reducing features...\n",
      "Train:  0.7138157894736842\n",
      "Test:  0.693069306930693\n",
      "Optimal number of features : 1\n",
      "Selected features: %s Index(['expected_attendance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Try decision trees\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(\"Selected features: %s\", xTrain.columns[rfecv.support_])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try the model as is:\n",
      "Train:  0.6348684210526315\n",
      "Test:  0.6270627062706271\n",
      "Reducing features...\n",
      "Train:  0.3651315789473684\n",
      "Test:  0.37293729372937295\n",
      "Optimal number of features : 177\n",
      "Selected features: %s Index(['gender', 'has_permission', 'unscheduled', 'can_call',\n",
      "       'can_have_alt_number', 'has_cv_print', 'venue', 'call_letter_shared',\n",
      "       'expected_attendance', 'Year',\n",
      "       ...\n",
      "       'native_loc_tirupati', 'native_loc_trichy', 'native_loc_trivandrum',\n",
      "       'native_loc_tuticorin', 'native_loc_vellore', 'native_loc_vijayawada',\n",
      "       'native_loc_visakapatinam', 'native_loc_warangal',\n",
      "       'marital_status_married', 'marital_status_single'],\n",
      "      dtype='object', length=177)\n"
     ]
    }
   ],
   "source": [
    "#Try SVM Linear Classification, we preffer classification over regression because this is a classification problem\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.LinearSVC()\n",
    "\n",
    "print('Try the model as is:')\n",
    "test(model)\n",
    "\n",
    "rfecv = feature_selection.RFECV(estimator=model)\n",
    "\n",
    "print('Reducing features...')\n",
    "test(rfecv)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(\"Selected features: %s\", xTrain.columns[rfecv.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4189300411522634\n"
     ]
    }
   ],
   "source": [
    "#Try K-means\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2) #We set n_clusters=2 because we only care about 2 possible outcomes.\n",
    "\n",
    "#We use the full dataset as indicated in the course content.\n",
    "X = np.array(cleanData.drop(['observed_attendance'], 1).astype(float))\n",
    "X = preprocessing.scale(X)\n",
    "Y = np.array(cleanData['observed_attendance'])\n",
    "model.fit(X)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = model.predict(predict_me)\n",
    "    if prediction[0] == Y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6267087276550999, 1: 0.7083333333333334, 2: 0.5555555555555556, 3: 0.8421052631578947, 4: 0.5294117647058824, 5: 0.15384615384615385, 6: 0.5833333333333334, 7: 0.6666666666666666, 8: 0.75, 9: 0.5714285714285714, 10: 0.5, 11: 0.8333333333333334, 12: 1.0, 13: 1.0, 14: 0.4, 15: 0.6, 16: 1.0, 17: 0.0, 18: 0.75, 19: 0.25, 20: 0.75, 21: 0.75, 22: 1.0, 23: 1.0, 24: 1.0, 25: 1.0, 26: 0.6666666666666666, 27: 0.6666666666666666, 28: 0.6666666666666666, 29: 0.5, 30: 0.5, 31: 0.5, 32: 0.5, 33: 1.0, 34: 0.5, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.0, 41: 1.0, 42: 1.0, 43: 0.0, 44: 0.0, 45: 0.0, 46: 1.0, 47: 1.0, 48: 0.0, 49: 1.0, 50: 0.0, 51: 1.0, 52: 0.0, 53: 0.0, 54: 1.0, 55: 0.0, 56: 1.0, 57: 0.0, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 0.0, 64: 1.0, 65: 1.0, 66: 1.0, 67: 0.0, 68: 1.0, 69: 0.0, 70: 1.0, 71: 1.0, 72: 1.0, 73: 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Try Mean Shift\n",
    "from sklearn.cluster import MeanShift\n",
    "model = MeanShift()\n",
    "model.fit(X)\n",
    "\n",
    "labels = model.labels_\n",
    "cluster_centers = model.cluster_centers_\n",
    "\n",
    "#We make a copy of the dataframe so we can label it with the result of the model\n",
    "dataCopy = pd.DataFrame.copy(cleanData)\n",
    "\n",
    "dataCopy['cluster_group'] = np.nan\n",
    "\n",
    "for i in range(len(X)):\n",
    "    #We expect a warning from this line, we will ignore it.\n",
    "    dataCopy['cluster_group'].iloc[i] = labels[i]\n",
    "\n",
    "attendance_rates = {}\n",
    "\n",
    "n_clusters = len(np.unique(labels))\n",
    "for i in range(n_clusters):\n",
    "    temp_df = dataCopy[(dataCopy['cluster_group']==float(i))]\n",
    "    attendance_cluster = temp_df[(temp_df['observed_attendance']==1)]\n",
    "    attendance_rate = len(attendance_cluster)/len(temp_df)\n",
    "    attendance_rates[i] = attendance_rate\n",
    "\n",
    "#The results look very confusing, 73 cluster groups.\n",
    "#We could look deeper into the data and try to find the correlation between specific variables and the observed_attendance\n",
    "print(attendance_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
